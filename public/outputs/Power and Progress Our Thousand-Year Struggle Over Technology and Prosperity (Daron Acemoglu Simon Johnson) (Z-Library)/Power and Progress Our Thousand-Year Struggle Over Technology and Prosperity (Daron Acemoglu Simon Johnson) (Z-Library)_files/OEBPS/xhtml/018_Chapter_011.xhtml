<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

    <title>Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity</title>
    <meta content="urn:uuid:088af2eb-2978-493e-af7d-0a134c4155be" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
<link rel="stylesheet" type="text/css" href="../../page_styles.css"/>

  


<link href="../../calibreHtmlOutBasicCss.css" type="text/css" rel="stylesheet" />

</head>
<body>

<div class="calibreMeta">
  <div class="calibreMetaTitle">
  
  
    
    <h1>
      <a href="../../../Power and Progress Our Thousand-Year Struggle Over Technology and Prosperity (Daron Acemoglu Simon Johnson) (Z-Library).html">Power and Progress
</a>
    </h1>
    
    
  
  </div>
  <div class="calibreMetaAuthor">
    Daron Acemoglu;Simon Johnson;

  </div>
</div>

<div class="calibreMain">

  <div class="calibreEbookContent">
    
      <div class="calibreEbNavTop">
        
          <a href="017_Chapter_010.xhtml" class="calibreAPrev">previous page
</a>
        

        
          <a href="019_Chapter_012.xhtml" class="calibreANext">next page
</a>
        
      </div>
    

    
<div class="galley-rw">
<section epub:type="bodymatter chapter" id="sec-chapter18">
<p class="cn" id="ji_1664" lang="en-US"><a id="page-339"></a><a href="toc.xhtml#toc18" id="toc_18">10</a></p>
<p class="ct" id="ji_1665" lang="en-US"><a href="toc.xhtml#toc18">Democracy Breaks</a></p>
<p class="ep-first-rule-above" id="ji_1666" lang="en-US">Social media’s history is not yet written, and its effects are not neutral.</p>
<p class="eps" id="ji_1667" lang="en-US">—<span class="eps-name-sc" lang="">Chris Cox</span>, head of product, Facebook, <span class="eps-year" lang="">2019</span></p>
<p class="ep1" id="ji_1668" lang="en-US">If everybody always lies to you, the consequence is not that you believe the lies, but rather that nobody believes anything any longer.</p>
<p class="eps_last-rule-below" id="ji_1669" lang="en-US">—Hannah Arendt, <span class="eps-year" lang="">1974</span><span class="charoverride1" lang=""> interview</span></p>
<p class="cotx" id="ji_1670" lang="en-US"><span class="_idgendropcap" lang="">O</span>n November 2, 2021, Chinese tennis star Peng Shuai posted on the social media site Weibo that she had been coerced into sex by a senior official. The message was removed within twenty minutes and never appeared again on Chinese social media. By the time it was removed, a number of users had taken screenshots of the post that were shared in foreign media. But access to these foreign outlets was also quickly censored. There was great interest in China about Peng Shuai, but few people were able to see the original post, and there was no public discussion.</p>
<p class="tx" id="ji_1671" lang="en-US">Such swift removal of politically sensitive information is the rule and not the exception in China, where the internet and social media are under constant surveillance. The Chinese government <a id="page-340"></a>reportedly spends an estimated $6.6 billion every year just on monitoring and censoring online content.</p>
<p class="tx" id="ji_1672" lang="en-US">The government also invests massively in other digital tools and especially AI for surveillance. This is most visible in Xinjiang province, where systematic data collection on Uighur Muslims dates back to the immediate aftermath of the 2009 July riots there, but have multiplied since 2014. The Communist Party has instructed several leading technology companies to develop tools for collecting, aggregating, and analyzing data on individual and household habits, communication patterns, jobs, spending, and even hobbies to be used as inputs into “predictive policing” against the eleven million inhabitants of the province who are seen as potential dissidents.</p>
<p class="tx" id="ji_1673" lang="en-US">Several of the major technology companies in China, including Ant Group (partly owned by Alibaba), the telecom giant Huawei, and some of the largest AI companies in the world such as SenseTime, CloudWalk, and Megvii, have cooperated with the government’s efforts to develop surveillance tools and their rollout in Xinjiang. Efforts at tracking people using their DNA are underway. AI technologies that recognize Uighurs on the basis of their facial features are also used routinely.</p>
<p class="tx" id="ji_1674" lang="en-US">What started in Xinjiang has since spread to the rest of China. Facial-recognition cameras are now widespread throughout the country, and the government has made steady progress toward introducing a national social credit system, which collects information on individuals and businesses to monitor their undesirable and untrustworthy activities. This, of course, includes dissent and subversive criticism of the government. According to the official planning document, a social credit system</p>
<p class="ext" id="ji_1675" lang="en-US">is founded on laws, regulations, standards and charters, it is based on a complete network covering the credit records of members of society and credit infrastructure, it is supported by the lawful application of credit information and a credit services system, its inherent requirements are establishing the idea of a sincerity culture, and carrying forward sincerity and traditional virtues, it uses encouragement to keep trust and <a id="page-341"></a>constraints against breaking trust as incentive mechanisms, and its objective is raising the honest mentality and credit levels of the entire society.</p>
<p class="tx" id="ji_1676" lang="en-US">Early versions of the system were developed alongside private-sector firms, including Alibaba, Tencent, and the ride-sharing company Didi, with the purported aim of differentiating between acceptable (to the authorities) behavior and unacceptable behavior—and limiting the mobility and other actions of transgressors. Since 2017, prototypes of the social credit system have been implemented in dozens of major cities, including Hangzhou, Chengdu, and Nanjing. According to the Supreme People’s Court, “Defaulters [on court orders] had been restrained from purchasing about 27.3 million plane tickets and nearly 6 million train tickets so far [July 9, 2019].” Some commentators have come to view the Chinese model and its social credit system as a prototype for a new kind of “digital dictatorship,” in which authoritarian rule is maintained by intense surveillance and data collection.</p>
<p class="tx" id="ji_1677" lang="en-US">Ironically, this is exactly the opposite of what many thought would be the effects of the internet and social media on political discourse and democracy. Online communication was promised to unleash the wisdom of the crowds, as different perspectives communicated and competed freely, enabling the truth to triumph. The internet was supposed to make democracies stronger and put dictatorships on the defensive as it revealed information on corruption, repression, and abuses. Wikis, such as the now infamous WikiLeaks, were viewed as steps toward democratizing journalism. Social media would do all the above and better by facilitating open political discourse and coordination among citizens.</p>
<p class="tx" id="ji_1678" lang="en-US">Early evidence seemed to bear this out. On January 17, 2001, text messages were used to coordinate protests in the Philippines against its Congress, which had decided to disregard critical evidence against President Joseph Estrada in his impeachment trial. As messages moved from one user to another, more than a million people arrived in downtown Manila to object to congresspeople’s complicity in Estrada’s corruption and crimes. After the capital <a id="page-342"></a>was brought to a standstill, the legislators reversed their decision, and Estrada was impeached.</p>
<p class="tx" id="ji_1679" lang="en-US">Less than a decade later, it was social media’s turn. Facebook and Twitter were used by protesters during the Arab Spring, helping to topple long-ruling autocrats Zine El Abidine Ben Ali in Tunisia and Hosni Mubarak in Egypt. One of the leaders of the Egyptian protests and a computer engineer at Google, Wael Ghonim, summarized both the mood among some of the protesters and the optimism in the tech world when he said in an interview: “I want to meet Mark Zuckerberg one day and thank him, actually. This revolution started—well, a lot of this revolution started on Facebook. If you want to liberate a society, just give them the Internet. If you want to have a free society, just give them Internet.” A cofounder of Twitter adopted the same interpretation of its own role, claiming “Some Tweets may facilitate positive change in a repressed country.…”</p>
<p class="tx" id="ji_1680" lang="en-US">Policy makers concurred. Secretary of State Hillary Rodham Clinton declared in 2010 that internet freedom would be a key pillar of her strategy for spreading democracy around the world.</p>
<p class="tx" id="ji_1681" lang="en-US">With these hopes, how did we end up in a world in which digital tools are powerful weapons in the hands of autocrats for suppressing information and dissent, and social media has become a hotbed of misinformation, manipulated not just by authoritarian governments but also by extremists from both the Right and the Left?</p>
<p class="tx" id="ji_1682" lang="en-US">In this chapter we argue that the pernicious effects of digital technologies and AI on politics and social discourse were not inevitable and resulted from the specific way in which these technologies were developed. Once these digital tools started being used primarily for massive data collection and processing, they became potent tools in the hands of both governments and companies interested in surveillance and manipulation. As people became more disempowered, top-down control intensified in both autocratic and democratic countries, and new business models based on monetizing and maximizing user engagement and outrage flourished.</p>
<p class="h1" id="ji_1683" lang="en-US"><a id="page-343"></a>A Politically Weaponized System of Censorship</p>
<p class="cotx" id="ji_1684" lang="en-US">It was never easy to be in the opposition in Communist China. In what many interpreted as a partial relaxation of the repression that had already taken millions of lives, Chairman Mao declared in 1957, “Let a hundred flowers bloom,” allowing criticism of the Communist Party. But hopes that this meant more accepting attitudes toward dissent were soon dashed. Mao initiated a vigorous “Anti-Rightist” campaign, and those who heeded his earlier invitation and attempted to express their critical views were rounded up, imprisoned, and tortured. At least five hundred thousand people were executed between 1957 and 1959.</p>
<p class="tx" id="ji_1685" lang="en-US">But by the late 1970s and early 1980s, things were looking very different. Mao had died in 1976, and the hard-liners, including his wife, Jiang Qing, and three of her Communist Party associates, commonly known as the “Gang of Four,” lost the ensuing power struggle and were sidelined. Deng Xiaoping, who was one of the revolution’s leaders, a successful general during the civil war, the architect of the Anti-Rightist campaign, secretary-general and vice premier, and later purged by Mao, came back on the scene and took charge in 1978. Deng reinvented himself as a reformer and attempted a major economic restructuring of China.</p>
<p class="tx" id="ji_1686" lang="en-US">This period witnessed a loosening of the power of the Communist Party. New independent media outlets sprang up, and some were openly critical of the party. Various grassroots movements also started during this period, including pro-democracy student movements and initiatives in the countryside to defend the rights of regular people against land grabs.</p>
<p class="tx" id="ji_1687" lang="en-US">Hopes of a more open society crumbled, once more, during the Tiananmen Square massacre in 1989. In the relatively more permissive days of the 1980s, demands for greater freedoms and reforms had built up in the cities and especially among the students. A major wave of student demonstrations had already taken place in 1986, with demands for democracy, greater freedom of speech, and economic liberalization. The hard-liners blamed the <a id="page-344"></a>pro-reform general secretary of the party, Hu Yaobang, for being soft on the protesters and removed him from power.</p>
<p class="tx" id="ji_1688" lang="en-US">New protests broke out in April 1989 after Hu’s death from a heart attack. Hundreds of students from Peking University marched to Tiananmen Square, at the center of Beijing, separated by the Gate of Heavenly Peace (<span class="ital" lang="">Tiananmen</span>) from the Forbidden City. As their ranks swelled over the next several hours, the students drafted the “Seven Demands,” which included calls for affirming Hu Yaobang’s views on democracy and freedom as correct, ending press censorship and restrictions on demonstrations, and curbing corruption by state leaders and their families.</p>
<p class="tx" id="ji_1689" lang="en-US">As the government prevaricated on how to respond, support for the protests grew, especially after students began a hunger strike on May 13. As many as a million Beijing residents demonstrated in solidarity in the middle of May. Finally, Deng Xiaoping weighed in on the side of hard-liners and approved military action against the students. Martial law was declared on May 20, and in the next two weeks more than 250,000 troops were sent to Beijing to quell the unrest. By June 4, the protests were quashed, and the square was emptied. Independent sources estimate the death toll among protesters to have been as high as 10,000. Tiananmen Square was a turning point in the Communist Party’s determination to clamp down on the freedoms that had emerged in the 1980s and to limit opposition activities.</p>
<p class="tx" id="ji_1690" lang="en-US">All the same, the ability of the Communist Party to control dissent in the vast territories it controlled remained limited in the 1990s and most of the 2000s. The grassroots Weiquan movement, which brought together a large number of lawyers to defend victims of human rights abuses throughout China and advocate for environmental causes, housing rights, and freedom of speech, started in the early 2000s. One of the pro-democracy movements that gained highest visibility, Charter 08, led by the writer and activist Liu Xiaobo, published its platform in 2008 and proposed reforms that went far beyond the seven demands of the Tiananmen Square protests. They included a new constitution, election of all public officials, separation of powers, an independent <a id="page-345"></a>judiciary, guarantees for basic human rights, and extensive freedom for association, assembly, and expression.</p>
<p class="tx" id="ji_1691" lang="en-US">By 2010, public dissent had become much harder in China, with the internet a potent tool in the hands of the authorities to monitor and sanitize political discourse. The internet had arrived in China in 1994, and efforts at censorship started soon thereafter. The “Great Firewall,” aimed at limiting what Chinese citizens could view and with whom they could communicate, originated in 2002, was completed in 2009, and has been extended periodically since then.</p>
<p class="tx" id="ji_1692" lang="en-US">During the early 2010s, however, digital censorship had its limits. A major research effort collected and analyzed millions of social media posts across 1,382 Chinese websites and platforms in 2011 and then followed them up to see if they were removed by Chinese authorities. Results show that the Great Firewall was effective, but only up to a point. The authorities did not censor most of the (hundreds of thousands of) posts critical of the government or the party. Rather, they removed the much smaller subset of posts that were on sensitive topics, which posed a risk of large-scale response and the possibility of coalescing different opposition groups. For instance, the vast majority of posts about protests in Inner Mongolia or Zhengcheng province were removed quickly. Those about Bo Xilai (former mayor of Dalian, member of the Politburo, and being purged at the time) or Fang Binxing (father of the Great Firewall) were removed equally rapidly.</p>
<p class="tx" id="ji_1693" lang="en-US">Another team of researchers found that, the Great Firewall and the systematic censorship notwithstanding, social media communication still acted as a trigger for protests. Messaging over Weibo enabled coordination and geographic spread of protest activities. Already during this period, however, social media–mediated dissident activities were short-lived.</p>
<p class="tx" id="ji_1694" lang="en-US">The softer-touch censorship that allowed some critical messages to circulate ceased after 2014. Under the leadership of Xi Jinping, the government increased its demand for surveillance and related AI technologies first in Xinjiang and then throughout China. In 2017 it issued the “New Generation AI Development <a id="page-346"></a>Plan,” with a goal of global leadership in AI and a clear focus on the use of AI for surveillance. Since 2014, China’s spending on surveillance software and cameras and its share of global investment in AI have increased rapidly every year, now making up about 20 percent of worldwide AI spending. Researchers located in China now account for more AI-related patents than any other country.</p>
<p class="tx" id="ji_1695" lang="en-US">With better AI technologies came more intense surveillance, and in the words of the founder of <span class="ital" lang="">China Digital Times</span>, Xiao Qiang, “China has a politically weaponized system of censorship; it is refined, organized, coordinated and supported by the state’s resources. It’s not just for deleting something. They also have a powerful apparatus to construct a narrative and aim it at any target with huge scale.”</p>
<p class="tx" id="ji_1696" lang="en-US">Today, very few dissenting posts escape censorship on any major social media platforms, the Great Firewall covers almost all politically sensitive foreign websites, and there is little evidence of protests being coordinated on social media. The Chinese can no longer access most independent foreign media, including the <span class="ital" lang="">New York Times</span>, CNN, BBC, the<span class="ital" lang=""> Guardian</span>, and the <span class="ital" lang="">Wall Street Journal</span>. Major Western social media outlets and search engines, including Google, YouTube, Facebook, Twitter, Instagram, and various video-sharing blog sites, are also blocked.</p>
<p class="tx" id="ji_1697" lang="en-US">AI has significantly amplified the Chinese government’s ability to suppress dissent and circumvent political discourse and information, especially in the context of multimedia content and live chats.</p>
<p class="h1" id="ji_1698" lang="en-US">A Braver New World</p>
<p class="cotx" id="ji_1699" lang="en-US">By the 2010s, Chinese political discourse was already looking like George Orwell’s <span class="ital" lang="">1984</span>. By suppressing information and using systemic propaganda, the government attempted to tightly control the political narrative. When corruption investigations that touched upon high-level politicians or their families were prominently reported in the foreign press, government censorship ensured that the Chinese people did not see these details and were <a id="page-347"></a>instead bombarded with propaganda about the virtuousness of their leaders. </p>
<p class="tx" id="ji_1700" lang="en-US">Many people appeared to be at least partially convinced by indoctrination or at the very least did not dare admit that they thought this was propaganda. The Communist Party initiated a major reform of high school curriculums in 2001. The aim was to politically educate the nation’s youths. A 2004 memo on the reform was titled “Suggestions on Strengthening the Ideological and Moral Construction of Our Youths.” The new textbooks, which started being rolled out in 2004, had a more nationalistic account of history and stressed the authority and virtues of the Communist Party. They criticized Western democracies and argued that China’s political system was superior.</p>
<p class="tx" id="ji_1701" lang="en-US">Students exposed to the new textbooks professed to hold very different opinions than their peers in the same province who graduated before the textbooks were introduced. They also reported higher levels of trust in government officials and deemed the Chinese system to be more democratic than did the students who were not indoctrinated by the same textbooks. Whether they truly believed these things or simply internalized the fact that they were expected to offer these opinions when asked is harder to establish. Nevertheless, it is clear that their reported views were strongly affected by the propaganda they were subjected to.</p>
<p class="tx" id="ji_1702" lang="en-US">By the late 2010s, these tendencies were all significantly intensified. Digital censorship and propaganda meant that nationalism, unquestioning support for the government, and unwillingness to listen to critical news and opinions had become much more widespread among Chinese youths. After the massive investments in AI, the Great Firewall was also complemented by constant surveillance using data gathered on all Chinese platforms and workplaces. In such an environment, would Chinese university students even want to access foreign media sources if they could? This is the question that two researchers set out to explore in an ambitious study. The answer they found was surprising, even to themselves.</p>
<p class="tx" id="ji_1703" lang="en-US">The Great Firewall had one weakness in the mid-2010s. It blocked Chinese users’ access to foreign media and websites <a id="page-348"></a>by using their IP addresses, which indicated whether they were located in mainland China. But VPNs (virtual private networks) could be used to hide IP addresses, allowing users in mainland China to access censored websites. The government had not explicitly banned VPNs, and information on websites visited by using them was unavailable to the authorities, making such a work-around fairly safe. (Things have changed since then, though, with private use of VPNs banned and all VPN providers having to register with the government.)</p>
<p class="tx" id="ji_1704" lang="en-US">In a cleverly designed experiment, the two researchers offered university students in Beijing free VPN access (and sometimes extra encouragements via newsletters and other means) so that they could access Western news outlets for a period of eighteen months between 2015 and 2017. Students receiving these additional encouragements visited Western media, were interested in the news, and, once having started doing so, continued to get news from foreign sources. Their survey responses indicate that they understood and believed the information, changed their political opinions, and became more critical of the Chinese government. They also expressed much more sympathy for democratic institutions.</p>
<p class="tx" id="ji_1705" lang="en-US">Nonetheless, without the extra encouragements, the vast majority of the students had no interest in visiting foreign websites and did not even want free VPN access. They were so convinced by the propaganda in schools and in the Chinese media that there was no relevant or reliable information about China in Western sources that they did not really need to be actively censored. They had already internalized the censorship.</p>
<p class="tx" id="ji_1706" lang="en-US">The researchers interpreted this finding as Aldous Huxley’s<span class="ital" lang=""> Brave New World</span> rather than George Orwell’s<span class="ital" lang=""> 1984</span>. In the words of the social critic Neil Postman, “What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who would want to read one.”</p>
<p class="tx" id="ji_1707" lang="en-US">In Huxley’s dystopia, society is divided between rigidly segmented castes, going from alphas at the top to betas, gammas, <a id="page-349"></a>deltas, and all the way down to epsilons. But there is no more need for constant censorship and surveillance because “under a scientific dictator education will really work—with the result that most men and women will grow up to love their servitude and will never dream of revolution. There seems to be no good reason why a thoroughly scientific dictatorship should ever be overthrown.”</p>
<p class="h1" id="ji_1708" lang="en-US">From Prometheus to Pegasus</p>
<p class="cotx" id="ji_1709" lang="en-US">The use of digital tools to suppress dissent is not unique to China. Iran and Russia, among other dictatorships, have also used them to track and punish dissenters and stifle access to free information.</p>
<p class="tx" id="ji_1710" lang="en-US">Even before the Arab Spring, social media use in pro-democracy protests had come to international attention during Iran’s ultimately unsuccessful Green Revolution. Huge crowds (by some estimated to be as many as three million) poured into the streets to bring down President Mahmoud Ahmadinejad, who was believed to have rigged the 2009 election to remain in power. Many social media tools, including text messages and Facebook, were used in coordinating the protests.</p>
<p class="tx" id="ji_1711" lang="en-US">The protests were soon suppressed, and a large number of opposition figures and students were arrested. In the aftermath, Iranian internet censorship intensified. In 2012 the Supreme Council of Cyberspace was launched to oversee the internet and social media, and today almost all Western social media, various streaming services (including Netflix), and most Western news media are blocked in Iran.</p>
<p class="tx" id="ji_1712" lang="en-US">The evolution of social media’s role in politics and the resulting government crackdown in Russia are similar. The site VK (<span class="ital" lang="">VKontakte</span>) emerged as the most popular social media platform in the country and was already widely used by 2011. Electoral fraud in the December 4, 2011, parliamentary elections, documented on the internet with pictures of ballot stuffing and multiple votes cast by government supporters, ignited massive protests. Subsequent research found that protests were coordinated on <a id="page-350"></a>the platform and there were significantly larger antigovernment actions in cities where VK was more widely used.</p>
<p class="tx" id="ji_1713" lang="en-US">As in China and Iran, the protests acted as a trigger for increased government control and censorship of online activity in Russia. Systematic censorship has since intensified. The System for Operative Investigative Activities compels all telecommunication operators to install hardware provided by the Federal Security Service (FSB), which enables the FSB to monitor metadata or even content, and also block access, without a need for a warrant. After another round of protests in 2020, more dissident and news websites were blocked, VPN tools and the encrypted browser Tor were banned, and new, astronomical fines were introduced as a way of coercing companies to prevent access to illegal content, including social media posts and websites critical of the government. Although AI technologies are less important to Russian censorship efforts, their role has recently grown as well.</p>
<p class="tx" id="ji_1714" lang="en-US">Abuse of digital tools directed against opposition groups is not confined to dictatorships. In 2020 a list of about fifty thousand phone numbers was leaked to Forbidden Stories, an international organization striving to publish stories from and about journalists under repression around the globe. The numbers belonged to opposition politicians, human rights activists, journalists, and dissidents who were reportedly being hacked using the Pegasus spyware developed by the Israeli tech company NSO Group (named after the first names of its founders, Niv Karmi, Shalev Hulio, and Omri Lavie). (NSO denies any wrongdoing, saying that the software is provided only to “vetted government customers” and that these customers decide how to use it.)</p>
<p class="tx" id="ji_1715" lang="en-US">Pegasus is a “zero-click” software, meaning that it can be installed on mobile phones remotely and without requiring a user to click on any links—in other words, it can be installed without the user’s knowledge or consent. Its name comes from the winged horse, Pegasus, in Greek mythology, with reference to the broad class of software it belongs to (Trojan horse) and the fact that it flies rather than being manually installed. As we saw in <a href="009_Chapter_002.xhtml">Chapter 1</a>, today’s tech leaders are keen to stress the fire-like powers of <a id="page-351"></a>AI and portray themselves as the current-day Prometheus, gifting technology’s powers to humanity. But Pegasus, not Prometheus, is what we seem to have gotten from modern digital technologies.</p>
<p class="tx" id="ji_1716" lang="en-US">Pegasus can read text messages, listen to calls, determine location, remotely collect passwords, monitor online activity, and even take control of a phone’s camera and microphone. It is allegedly used routinely in many countries with authoritarian rulers, including Saudi Arabia, the United Arab Emirates, and Hungary. The journalist Jamal Khashoggi, who was later brutally murdered and dismembered, was allegedly under surveillance by Saudi Arabian agents using Pegasus. (The Saudi authorities have said this was a “rogue operation.”)</p>
<p class="tx" id="ji_1717" lang="en-US">Investigation of the numbers obtained by Forbidden Stories reveals systematic abuse of the software by many democratically elected governments as well. In Mexico, spyware was originally acquired as a weapon against drug cartels and deployed in the operation that led to the capture of the head of the Sinaloa cartel, El Chapo. But it was subsequently turned against journalists, lawyers investigating the massacre of forty-three students, and opposition parties, including one of the opposition’s leaders, Andrés Manuel López Obrador, who later became the country’s president. In India, Prime Minister Narendra Modi’s government uses the software even more extensively and has placed many important opposition leaders, student activists, journalists, election commissioners, and even heads of the country’s Central Bureau of Investigation under surveillance.</p>
<p class="tx" id="ji_1718" lang="en-US">Abuses using Pegasus went beyond developing-country governments. French president Emmanuel Macron’s phone was on the list, as were the numbers of several high-ranking US State Department officials.</p>
<p class="tx" id="ji_1719" lang="en-US">The United States does not need Pegasus for high-tech transgressions (although some of its security agencies did experiment with the software and also acted as an intermediary in its sale to the Djibouti government). On June 5, 2013, the world was awakened to revelations from Edward Snowden, first published in the <span class="ital" lang="">Guardian</span> newspaper, about illegal data collection by the National <a id="page-352"></a>Security Agency (NSA). The NSA cooperated with Google, Microsoft, Facebook, Yahoo!, various other internet service providers, and telephone companies such as AT&amp;T and Verizon to scoop up huge amounts of data about American citizens’ internet searches, online communications, and phone calls. It also tapped communication by leaders of American allies, including Germany and Brazil. It collected data from satellites and underwater fiber-optic cables. Snowden described the reach of these programs by saying that when he was a contractor for the NSA, “I, sitting at my desk, certainly had the authorities to wiretap anyone, from you or your accountant, to a federal judge or even the President, if I had a personal e-mail.” Though unconstitutional and taking place without the knowledge or oversight of Congress, some of these activities were sanctioned by FISA (the Foreign Intelligence Surveillance Court).</p>
<p class="tx" id="ji_1720" lang="en-US">The United States is not China, and these activities had to be hidden from the news media and even from most lawmakers. When Snowden’s revelations broke out, there was a powerful reaction against the NSA and other agencies’ abusive data-collection strategies. But this was not enough to put a stop to most surveillance. Perhaps even worse, private companies such as Clearview AI have started collecting facial images from hundreds of millions of users and selling this information to law-enforcement agencies, with essentially no oversight from civil society or other institutions. There is nothing wrong in this, according to Clearview’s founder and CEO, who states, “Our belief is that this is the best use of the technology.”</p>
<p class="tx" id="ji_1721" lang="en-US">Pegasus spyware, snooping by NSA, and Clearview’s facial-recognition technology illustrate a deeper problem. Once out there, digital tools for extensive data collection will be adopted by many, if not most, governments to suppress opposition and better monitor their citizens. They will strengthen nondemocratic regimes and enable them to withstand opposition much more effectively. They could even create a slippery slope for democratic governments to become more authoritarian over time.</p>
<p class="tx" id="ji_1722" lang="en-US">Democracy dies in darkness. But it also struggles under the light provided by modern artificial intelligence.</p>
<p class="h1" id="ji_1723" lang="en-US"><a id="page-353"></a>Surveillance and the Direction of Technology</p>
<p class="cotx" id="ji_1724" lang="en-US">From the initial euphoria about the democratizing potential of the internet and social media, some have jumped to the polar opposite conclusion: digital tools are inherently antidemocratic. In the words of historian Yuval Noah Harari, “Technology favors tyranny.”</p>
<p class="tx" id="ji_1725" lang="en-US">Both of these binary perspectives are wrong. Digital technology is not pro-democratic or antidemocratic. Nor was there any necessity for AI technologies to be developed to empower governments to monitor media, censor information, and repress their citizens. All of this was a choice of direction for technology.</p>
<p class="tx" id="ji_1726" lang="en-US">We saw in <a href="017_Chapter_010.xhtml">Chapter 9</a> that digital technologies, which are almost by their nature highly general purpose, could have been used to further machine usefulness—for example, by creating new worker tasks or new platforms that multiplied human capabilities. It was the vision and the business model of large tech companies that pushed toward a primary focus on worker monitoring and job destruction through automation. The same is true when it comes to the use of AI as a tool in the hands of authoritarian governments and some purportedly democratic ones.</p>
<p class="tx" id="ji_1727" lang="en-US">The dreams of the internet and digital technologies empowering citizens against dictatorship were not completely surreal. Digital technologies can be used for encryption, making it impossible for authorities to snoop in private communications. Services such as VPNs can be used to thwart censorship. Search engines such as Tor are currently impossible for governments to decrypt (so far as we know) and hence offer greater levels of privacy and security. Nevertheless, early hopes of digital democratization have been dashed because the tech world put its effort where the money and power lie—with government censorship.</p>
<p class="tx" id="ji_1728" lang="en-US">It is thus a specific path—a low road—chosen by the tech community that intensifies data collection and surveillance. Although advances in large-scale processing of data using tools from machine learning have been important in these efforts, the real secret sauce in surveillance by governments and companies is massive amounts of data.</p>
<p class="tx" id="ji_1729" lang="en-US"><a id="page-354"></a>Once AI technologies strengthen authoritarian impulses, they create a vicious circle. As governments become more authoritarian, their demand for AI to track and control their population increases, and this pushes AI further in the direction of becoming a fully fledged monitoring technology.</p>
<p class="tx" id="ji_1730" lang="en-US">Since 2014, there has been, for example, a huge increase in demand from local Chinese governments for AI technologies that provide facial recognition and other types of monitoring. This demand seems to be triggered, in part, by local political unrest. Politicians want to increase policing and surveillance when they see discontent or protest activity brewing in their region. In the second half of the 2010s, massive protests, especially targeted against the national government, were all but impossible, although local protests were still taking place, and for a while, as we have seen earlier in this chapter, they were even coordinated on social media.</p>
<p class="tx" id="ji_1731" lang="en-US">By this point, however, AI tools were firmly on the side of the crackdowns, not of the protesters. Once empowered by AI technologies, local authorities become better at putting down and avoiding protests. Incidentally, even though the Chinese central government and local authorities are willing to hire large numbers of police officers, the increase in AI investments appears to reduce the need for using manpower to do the surveillance and even the actual repression of protesters.</p>
<p class="tx" id="ji_1732" lang="en-US">More strikingly, this demand from local governments affects the direction of innovation. Data on the universe of AI start-ups in China show that government demand for monitoring technologies fundamentally transforms subsequent innovation. AI firms contracting with Chinese local governments start shifting their research more and more toward facial recognition and other tracking technologies. Perhaps as a result of these incentives, China has emerged as a global leader in surveillance technologies, such as facial recognition, but lags in other areas, including natural-language processing, language-reasoning skills, and abstract reasoning.</p>
<p class="tx" id="ji_1733" lang="en-US">International experts rank the quality of AI research in China to be still significantly behind that of the United States in all <a id="page-355"></a>dimensions. There is one aspect in which China has an advantage, however: data.</p>
<p class="tx" id="ji_1734" lang="en-US">Chinese researchers work with much larger quantities of data and without the privacy restrictions that often limit the type of data Western researchers can access. The impact of local government contracts on the direction of AI research is particularly pronounced when local governments share vast amounts of data in their procurement contracts. With abundant data available to them without any strings and strong demand for surveillance technologies, AI start-ups were able to test and develop powerful applications that could track, monitor, and control citizens.</p>
<p class="tx" id="ji_1735" lang="en-US">There is a surveillance technology trap here: powerful and cash-rich governments that are intent on suppressing dissent demand AI technologies to control their population. The more they demand them, the more researchers produce them. The more AI moves in this repressive direction, the more attractive it becomes to authoritarian (or wannabe authoritarian) governments.</p>
<p class="tx" id="ji_1736" lang="en-US">Indeed, Chinese start-ups are now exporting their AI products targeted at monitoring and repression to other nondemocratic governments. The Chinese tech giant Huawei, one of the main beneficiaries of unrestricted access to data and financial incentives to develop snooping technologies, exported these tools to fifty other countries. In <a href="017_Chapter_010.xhtml">Chapter 9</a> we saw how AI-based automation developed in technologically advanced countries will affect the rest of the world, with significant potential downsides for most workers. The same is true for AI-based surveillance: most citizens, wherever they are around the world, are finding it harder and harder to escape repression.</p>
<p class="h1" id="ji_1737" lang="en-US">Social Media and Paper Clips</p>
<p class="cotx" id="ji_1738" lang="en-US">Internet censorship and even high-tech spyware may say nothing about the potential of social media as a tool to improve political discourse and coordinate opposition to the worst regimes in the world. That several dictatorships have used new technologies to repress their populations should not surprise anybody. That the United States has done the same is also understandable when you <a id="page-356"></a>think about its security services’ long tradition of lawless behavior that has only been amplified with the “War on Terror.” Perhaps the solution is to double down on social media and allow more connectivity and unencumbered messaging to shine a brighter light on abuses. Alas, AI-powered social media’s current path appears almost as pernicious for democracy and human rights as top-down internet censorship.</p>
<p class="tx" id="ji_1739" lang="en-US">The paper-clip parable is a favorite tool of computer scientists and philosophers for emphasizing the dangers that superintelligent AI will pose if its objectives are not perfectly aligned with those of humanity. The thought experiment presupposes an unstoppably powerful intelligent machine that gets instructions to produce more paper clips and then uses its considerable capabilities to excel in meeting this objective by coming up with new methods to transform the entire world into paper clips. When it comes to the effects of AI on politics, it may be turning our institutions into paper clips, not thanks to its superior capabilities but because of its mediocrity.</p>
<p class="tx" id="ji_1740" lang="en-US">By 2017, Facebook was so popular in Myanmar that it came to be identified with the internet itself. The twenty-two million users, out of a population of fifty-three million, were fertile ground for misinformation and hate speech. One of the most ethnically diverse countries in the world, Myanmar is home to 135 officially recognized distinct ethnicities. Its military, which has ruled the country with an iron fist since 1962, with a brief period of parliamentary democracy under military tutelage between 2015 and 2020, has often stoked ethnic hatred among the majority-Buddhist population. No other group has been as often targeted as the Rohingya Muslims, whom the government propaganda portrays as foreigners, even though they have lived there for centuries. Hate speech against the Rohingya has been commonplace in government-controlled media.</p>
<p class="tx" id="ji_1741" lang="en-US">Facebook arrived into this combustible mix of ethnic tension and incendiary propaganda in 2010. From then on it expanded rapidly. Consistent with Silicon Valley’s belief in the superiority of algorithms to humans and despite its huge user base, Facebook employed only one person who monitored Myanmar and spoke <a id="page-357"></a>Burmese but not most of the other hundred or so languages used in the country.</p>
<p class="tx" id="ji_1742" lang="en-US">In Myanmar, hate speech and incitement were rife on Facebook from the beginning. In June 2012 a senior official close to the country’s president, Thein Sein, posted this on his Facebook page:</p>
<p class="ext" id="ji_1743" lang="en-US">It is heard that Rohingya Terrorists of the so-called Rohingya Solidarity Organization are crossing the border and getting into the country with the weapons. That is Rohingyas from other countries are coming into the country. Since our Military has got the news in advance, we will eradicate them until the end! I believe we are already doing it.</p>
<p class="cotx" id="ji_1744" lang="en-US">The post continued: “We don’t want to hear any humanitarian issues or human rights from others.” The post was not only whipping up hatred against the Muslim minority but also amplifying the false narrative that Rohingya were coming into the country from the outside.</p>
<p class="tx" id="ji_1745" lang="en-US">In 2013 the Buddhist monk Ashin Wirathu, dubbed as the face of Buddhist terror by <span class="ital" lang="">Time </span>magazine that same year, was posting Facebook messages calling the Rohingya invading foreigners, murderers, and a danger to the country. He would eventually say, “I accept the term extremist with pride.”</p>
<p class="tx" id="ji_1746" lang="en-US">Calls from activists and international organizations to Facebook to clamp down on misleading information and incendiary posts kept growing. A Facebook executive admitted, “We agree that we can and should do more.” Yet by August 2017, whatever Facebook was doing was far from enough to monitor hate speech. The platform had become the chief medium for organizing what the United States would eventually call a genocide.</p>
<p class="tx" id="ji_1747" lang="en-US">The popularity of hate speech on Facebook in Myanmar should not have been a surprise. Facebook’s business model was based on maximizing user engagement, and any messages that garnered strong emotions, including of course hate speech and provocative misinformation, were favored by the platform’s algorithms because they triggered intense engagement from thousands, sometimes hundreds of thousands, of users.</p>
<p class="tx" id="ji_1748" lang="en-US"><a id="page-358"></a>Human rights groups and activists brought up these concerns about mounting hate speech and the resulting atrocities to Facebook’s leadership as early as 2014, with little success. The problem was at first ignored and activists were stonewalled, while the amount of false, incendiary information against the Rohingya continued to balloon. So did evidence that hate crimes, including murders of the Muslim minority, were being organized on the platform. Although the company was reluctant to do much on the hate-crime problem, this was not because it did not care about Myanmar. When the country’s government closed down Facebook, its executives immediately jumped into action, fearing that the shutdown might drive away some of its twenty-two million users in the country.</p>
<p class="tx" id="ji_1749" lang="en-US">Facebook also accommodated the government’s demands in 2019 to label four ethnic organizations as “dangerous” and ban them from the platform. These websites, though associated with ethnic separatist groups, such as the Arakan Army, the Kachin Independence Army, and the Myanmar National Democratic Alliance Army, were some of the main repositories of photos and other proofs of murders and other atrocities by the army and extremist Buddhist monks.</p>
<p class="tx" id="ji_1750" lang="en-US">When Facebook finally responded to earlier pressure, its solution was to create “stickers” to identify potential hate speech. The stickers would allow users to post messages that included harmful or questionable content but would warn them, “Think before you share” or “Don’t be the cause of violence.” However, it turned out that just like a dumb version of the paper clip–obsessed AI program, Facebook’s algorithm was so intent on maximizing engagement that it registered harmful posts as being more popular because people were engaging with the content to flag it as harmful. The algorithm then recommended this content more widely in Myanmar, further exacerbating the spread of hate speech.</p>
<p class="tx" id="ji_1751" lang="en-US">Myanmar’s lessons do not seem to have been well learned by Facebook. In 2018 similar dynamics started being played out in Sri Lanka, with posts on Facebook inciting violence against Muslims. Human rights groups reported the hate speech, but to no avail. In the assessment of a researcher and activist, “There’s incitements to <a id="page-359"></a>violence against entire communities and Facebook says it doesn’t violate community standards.”</p>
<p class="tx" id="ji_1752" lang="en-US">Two years later, in 2020, it was India’s turn. Facebook executives overrode calls from their employees and refused to remove Indian politician T. Raja Singh, who was calling for Rohingya Muslim immigrants to be shot and encouraging the destruction of mosques. Many were indeed destroyed in anti-Muslim riots in Delhi that year, which also killed more than fifty people.</p>
<p class="h1" id="ji_1753" lang="en-US">Misinformation Machine</p>
<p class="cotx" id="ji_1754" lang="en-US">The problems of hate speech and misinformation in Myanmar are paralleled by how Facebook has been used in the United States, and for the same reason: hate speech, extremism, and misinformation generate strong emotions and increase engagement and time spent on the platform. This enables Facebook to sell more individualized digital ads.</p>
<p class="tx" id="ji_1755" lang="en-US">During the US presidential election of 2016, there was a remarkable increase in the number of posts with either misleading information or demonstrably false content. Nevertheless, by 2020, 14 percent of Americans viewed social media as their primary source of news, and 70 percent reported receiving at least some of their news from Facebook and other social media outlets.</p>
<p class="tx" id="ji_1756" lang="en-US">These stories were not just a sideshow. A study of misinformation on the platform concluded that “falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information.” Many of the blatantly misleading posts went viral because they kept being shared. But it was not just users propagating falsehoods. Facebook’s algorithms were elevating these sensational articles ahead of both less politically relevant posts and information from trusted media sources.</p>
<p class="tx" id="ji_1757" lang="en-US">During the 2016 presidential election, Facebook was a major conduit for misinformation, especially for right-leaning users. Trump supporters often reached sites propagating misinformation from Facebook. There was less traffic going from social media to traditional media. Worse, recent research documents that people tend to believe posts with misinformation because they are <a id="page-360"></a>bad at remembering where they saw a piece of news. This may be particularly significant because users often receive unreliable and sometimes downright false information from their like-minded friends and acquaintances. They are also unlikely to be exposed to contrarian voices in these echo chamber–like environments.</p>
<p class="tx" id="ji_1758" lang="en-US">Echo chambers may be an inevitable by-product of social media. But it has been known for more than a decade that they are exacerbated by platform algorithms. Eli Pariser, internet activist and executive director of MoveOn.org, reported in a TED talk in 2010 that although he followed many liberal and conservative news sites, after a while he noticed he was directed more and more to liberal sites because the algorithm had noticed he was a little more likely to click on them. He coined the term <span class="ital" lang="">filter bubble</span> to describe how algorithm filters were creating an artificial space in which people heard only voices that were already aligned with their political views.</p>
<p class="tx" id="ji_1759" lang="en-US">Filter bubbles have pernicious effects. Facebook’s algorithm is more likely to show right-wing content to users who have a right-leaning ideology, and vice versa for left-wingers. Researchers have documented that the resulting filter bubbles exacerbate the spread of misinformation on the social media site because people are influenced by the news items they see. These filter-bubble effects go beyond social media. Recent research that incentivized some regular Fox News users to watch CNN found that exposure to CNN content had a moderating effect on their beliefs and political attitudes across a range of issues. The main reason for this effect appears to be that Fox News was giving a slanted presentation of some facts and concealing others, pushing users in a more right-wing direction. There is growing evidence that these effects are even stronger on social media.</p>
<p class="tx" id="ji_1760" lang="en-US">Although there were hearings and media reaction to Facebook’s role in the 2016 election, not much had changed by 2020. Misinformation multiplied on the platform, some of it propagated by President Donald Trump, who frequently claimed that mail-in ballots were fraudulent and that noncitizen immigrants were voting in droves. He repeatedly used social media to call a stop to the vote count.</p>
<p class="tx" id="ji_1761" lang="en-US"><a id="page-361"></a>In the run-up to the election, Facebook was also mired in controversy because of a doctored video of House Speaker Nancy Pelosi, giving the impression that she was drunk or ill, slurring her words and sounding unwell in general. The fake video was promoted by Trump allies, including Rudy Giuliani, and the hashtag #DrunkNancy began to trend. It soon went viral and attracted more than two million views. Crazy conspiracy theories, such as those that came from QAnon, circulated uninterrupted in the platform’s filter bubbles as well. Documents provided to the US Congress and the Securities and Exchange Commission by former Facebook employee Frances Haugen reveal that Facebook executives were often informed of these developments.</p>
<p class="tx" id="ji_1762" lang="en-US">As Facebook came under increasing pressure, its vice president of global affairs and communications, former British deputy prime minister Nick Clegg, defended the company’s policies, stating that a social media platform should be viewed as a tennis court: “Our job is to make sure the court is ready—the surface is flat, the lines painted, the net at the correct height. But we don’t pick up a racket and start playing. How the players play the game is up to them, not us.”</p>
<p class="tx" id="ji_1763" lang="en-US">In the week following the election, Facebook introduced an emergency measure, altering its algorithms to stop the spread of right-wing conspiracy theories claiming that the election was in reality won by Trump but stolen because of illegal votes and irregularities at ballot boxes. By the end of December, however, Facebook’s algorithm was back to its usual self, and the “tennis court” was open for a rematch of the 2016 fiasco.</p>
<p class="tx" id="ji_1764" lang="en-US">Several extremist right-wing groups as well as Donald Trump continued to propagate falsehoods, and we now know that the January 6, 2021, insurrection was partly organized using Facebook and other social media sites. For example, members of the far-right militia group Oath Keepers used Facebook to discuss how and where they would meet, and several other extremist groups live-messaged each other over the platform on January 6. One of the Oath Keepers’ leaders, Thomas Caldwell, is alleged to have posted updates as he entered the Capitol and to have received information over the platform on how to navigate <a id="page-362"></a>the building as well as to incite violence toward lawmakers and police.</p>
<p class="tx" id="ji_1765" lang="en-US">Misinformation and hate speech are not confined to Facebook. Around 2016, YouTube emerged as one of the most potent recruitment grounds for the Far Right. In 2019, Caleb Cain, a twenty-six-year-old college dropout, made a video about YouTube explaining how he had been radicalized on the platform. As he said, “I fell down the alt-right rabbit hole.” Cain explained how he “kept falling deeper and deeper into this” as he watched more and more radical content recommended by YouTube’s algorithms.</p>
<p class="tx" id="ji_1766" lang="en-US">Journalist Robert Evans studied how scores of ordinary people around the country were recruited by these groups, and he concluded that the groups themselves mentioned YouTube most often on their website: “15 out of 75 fascist activists we studied credited YouTube videos with their red-pilling.” (“Red-pilling” refers to the lingo that these groups used, with reference to the movie <span class="ital" lang="">The</span> <span class="ital" lang="">Matrix</span>: accepting the truths propagated by these far-right groups was the equivalent of taking the red pill in the movie.)</p>
<p class="tx" id="ji_1767" lang="en-US">YouTube’s algorithmic choices and intent to boost watch time on the platform were critical for these outcomes. To increase watch time, in 2012 the company modified its algorithm to give more weight to the time that users spend watching rather than just clicking on content. This algorithmic tweak started favoring videos that people became glued to, including some of the more incendiary extremist content, the sort that Cain became hooked on.</p>
<p class="tx" id="ji_1768" lang="en-US">In 2015 YouTube engaged a research team from its parent company’s AI division, Google Brain, to improve the platform’s algorithm. New algorithms then led to more pathways for users to become radicalized—while, of course, spending more time on the platform. One of Google Brain’s researchers, Minmin Chen, boasted in an AI conference that the new algorithm was successfully altering user behavior: “We can really lead the users towards a different state, versus recommending content that is familiar.” This was ideal for fringe groups trying to radicalize people. It meant that users watching a video on 9/11 would <a id="page-363"></a>be quickly recommended content on 9/11 conspiracies. With about 70 percent of all videos watched on the platform coming from algorithm recommendations, this meant plenty of room for misinformation and manipulation to pull users into the rabbit hole.</p>
<p class="tx" id="ji_1769" lang="en-US">Twitter was no different. As the favorite communication medium of former president Trump, it became an important tool for communication between right-wingers (and separately among left-wingers as well). Trump’s anti-Muslim tweets were widely disseminated and subsequently caused not just more anti-Muslim and xenophobic posts on the platform but also actual hate crimes against Muslims, especially in states where the president had more followers.</p>
<p class="tx" id="ji_1770" lang="en-US">Some of the worst language and consistent hate speech were propagated on other platforms, such as 4chan, 8chan, and Reddit, including its various sub-Reddits such as The_Donald (where conspiracy theories and misinformation related to Donald Trump originate and circulate), Physical_Removal (advocating the elimination of liberals), and several others with explicitly racist names that we prefer not to print here. In 2015 the Southern Poverty Law Center named Reddit as the platform hosting “the most violent racist” content on the internet.</p>
<p class="tx" id="ji_1771" lang="en-US">Was it unavoidable that social media should have become such a cesspool? Or was it some of the decisions that leading tech companies made that brought us to this sorry state? The truth is much closer to the latter, and in fact also answers the question posed in <a href="017_Chapter_010.xhtml">Chapter 9</a>: Why has AI become so popular even if it is not massively increasing productivity and outperforming humans?</p>
<p class="tx" id="ji_1772" lang="en-US">The answer—and the reason for the specific path that digital technologies took—is the revenues that companies that collect vast amounts of data can generate using individually targeted digital advertising. But digital ads are only as good as the attention that people pay to them, so this business model meant that platforms strove to increase user engagement with online content. The most effective way of doing this turned out to be cultivating strong emotions such as outrage or indignation.</p>
<p class="h1" id="ji_1773" lang="en-US"><a id="page-364"></a>The Ad Bargain</p>
<p class="cotx" id="ji_1774" lang="en-US">To understand the roots of misinformation on social media, we must turn to Google’s origin story.</p>
<p class="tx" id="ji_1775" lang="en-US">The internet was flourishing before Google, but the available search engines were not helping. What makes the internet so special is its amazing size, with the number of websites estimated at 1.88 billion in 2021. Sifting through these many websites and finding the relevant information or products was bound to be a challenge.</p>
<p class="tx" id="ji_1776" lang="en-US">The idea of early search engines was familiar to anyone who has used a book index: find all the occurrences of a given search word. If you wanted to find where the Neolithic Age was discussed in a book, you would look at the index and see the list of pages where the word <span class="ital" lang="">Neolithic</span> appeared. It worked well because a given word appeared a limited number of times, making the method of “exhaustive search” among all of the indicated pages feasible and quite effective. But imagine that you are looking into the index of an enormously large book, such as the internet. If you get the list of the instances in which the word <span class="ital" lang="">Neolithic</span> is mentioned in this humongous book, it may be hundreds of thousands of times. Good luck with exhaustive search!</p>
<p class="tx" id="ji_1777" lang="en-US">Of course, the problem is that many of these mentions are not that relevant, and only one or two websites would be the authoritative sources in which one can obtain the necessary information about the Neolithic Age and how, say, humans transitioned to settled life and permanent agriculture. Only a way of prioritizing the more important mentions would enable the relevant information to be quickly retrieved. But this is not what the early search engines were capable of doing.</p>
<p class="tx" id="ji_1778" lang="en-US">Enter two brash, smart young men, Larry Page and Sergey Brin. Page was a graduate student, working with the famous computer scientist Terry Winograd at Stanford, and Sergey Brin was his friend. Winograd, an early enthusiast for the currently dominant paradigm of AI, had by that point changed his mind and was working on problems in which human and machine knowledge could be combined, very much as Wiener, Licklider, and <a id="page-365"></a>Engelbart had envisaged. The internet, as we have seen, was an obvious domain for such a combination because its raw material was content and knowledge created by humans but it needed to be navigated by algorithms.</p>
<p class="tx" id="ji_1779" lang="en-US">Page and Brin came up with a better way of achieving this combination, a true human-machine interaction in some sense: Humans were the best judge of which websites were more relevant, and search algorithms were excellent at collecting and processing link information. Why not let human choices about linkages guide how search algorithms should prioritize relevant websites?</p>
<p class="tx" id="ji_1780" lang="en-US">This was at first a theoretical idea—the realization that this could be done. Then came the algorithmic solution of how to do it. This was the basis of their revolutionary PageRank algorithm (“Page” here reputedly refers both to Larry Page and the fact that pages are being ranked). Among the relevant pages, the idea was to prioritize those that received more links. So rather than using some ad hoc rules to decide which ones of the pages that have the word <span class="ital" lang="">Neolithic</span> should be suggested, the algorithm would rank these pages according to how many incoming links they received. The more popular pages would be more highly ranked. But why stop there? If a page received links from other highly ranked pages, that would be more informative about its relevance. To encapsulate this insight, Brin and Page developed a recursive algorithm where each page has a rank, and this rank is determined by how many other highly ranked pages were linking to it (“recursive” means that each page’s rank depends on the ranks of all others). With millions of websites, calculating these ranks is no trivial matter, but it was already feasible by the 1990s.</p>
<p class="tx" id="ji_1781" lang="en-US">Ultimately, how the algorithm computes the results is secondary. The important breakthrough here was that Page and Brin had come up with a way of using human insights and knowledge, as encapsulated in their subjective evaluations of which other pages were relevant, to improve a key machine task: ranking search outcomes. Brin and Page’s 1998 paper, titled “The Anatomy of a Large-Scale Hypertextual Web Search Engine,” starts with this sentence: “In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure <a id="page-366"></a>present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems.”</p>
<p class="tx" id="ji_1782" lang="en-US">Page and Brin understood that this was a major breakthrough but did not have a clear plan for commercializing it. Larry Page is quoted as saying that “amazingly, I had no thought of building a search engine. The idea wasn’t even on the radar.” But by the end of the project, it was clear that they had a winner on their hands. If they could build this search engine, it would tremendously improve how the World Wide Web functioned.</p>
<p class="tx" id="ji_1783" lang="en-US">Thus arrived Google, as a company. Page and Brin’s first idea was to sell or license their software to others. But their initial attempts did not gain traction, partly because other major tech companies were already locked into their own approaches or were prioritizing other areas: at that point search was not seen as a major moneymaker. Yahoo!, the leading platform at that time, showed no interest in Page and Brin’s algorithm.</p>
<p class="tx" id="ji_1784" lang="en-US">This changed in 1998, when a tech investor, Andy Bechtolsheim, entered the scene. Bechtolsheim met with Page and Brin and immediately got the promise of the new technology, if they had the right way of monetizing it. Bechtolsheim knew what that would be—ads.</p>
<p class="tx" id="ji_1785" lang="en-US">Selling ads was not what Page and Brin were planning or even considering. But right away Bechtolsheim changed the game with a check for $100,000 to Google Inc., even though Google was not yet “incorporated.” Soon the company was incorporated, the ad potential of the new technology became clear, and a lot more money poured in. A new business model was born.</p>
<p class="tx" id="ji_1786" lang="en-US">The company introduced AdWords in 2000, a platform that sold advertising to be displayed to users searching for websites using Google. The platform was based on an extension of well-known auction models used in economics, and it rapidly auctioned off the most valuable (highly visible) places on the search screen. Prices depended on how much potential advertisers bid and how many clicks their ads received.</p>
<p class="tx" id="ji_1787" lang="en-US">In 1998, or even in 2000, almost nobody was thinking about big data and AI. However, AI tools applied to large amounts of <a id="page-367"></a>data would soon mean a lot of information for companies to target ads to users according to what they were interested in. AI quickly revolutionized Google’s already-successful monetization model. This meant, in particular, that Google could track which websites were visited from the user’s unique IP address, and thus direct individualized ads for this specific user. Hence, users looking at Caribbean beaches would receive ads from airlines, travel agencies, and hotels, and those browsing clothes or shoes would be bombarded with ads from relevant retailers.</p>
<p class="tx" id="ji_1788" lang="en-US">The value of targeting in advertising cannot be overstated. The perennial problem of the ad industry is encapsulated in a saying that dates to the late 1800s: “I know half of my advertising is wasted, but I just don’t know which half.” Early internet advertising was afflicted by this problem. Ads from a menswear retailer would be shown to all users on a particular platform, say the Pandora music program, but half of the users would be women, and even most men would have no interest in buying clothes online at that point. With targeting, ads could be sent only to those who have demonstrated interest in making a purchase—for example, having visited a clothing store website or browsed some fashion items elsewhere. Targeting revolutionized digital advertising, but as with many revolutions, there would be plenty of collateral damage.</p>
<p class="tx" id="ji_1789" lang="en-US">Google soon accelerated its data collection by offering a range of sophisticated free products, such as Gmail and Google Maps, which enabled the company to learn much more about users’ preferences beyond the items they were searching for and their exact location. It also acquired YouTube. Now ads could be catered much more specifically for each user depending on their entire profile of purchases, activities, and location, boosting profitability. The results were striking, and in 2021 the vast majority of Google’s (or its parent company Alphabet’s) $65.1 billion revenue came from ads.</p>
<p class="tx" id="ji_1790" lang="en-US">Google and other companies figured out how to make a lot of money from ads, and this not only explains the emergence of a new business model. It also answers a fundamental question we posed in <a href="017_Chapter_010.xhtml">Chapter 9</a>: If it often leads to so-so automation, why is <a id="page-368"></a>there so much enthusiasm about AI? The answer is largely about massive data collection and targeted ads, and there was much more of both to come.</p>
<p class="h1" id="ji_1791" lang="en-US">The Socially Bankrupt Web</p>
<p class="cotx" id="ji_1792" lang="en-US">What Google can learn about its users from the metadata of their email activity and location pales in comparison with what some people are willing to share with their friends and acquaintances about their activities, intentions, desires, and views. Social media was bound to put the targeted-ad business model into a higher gear.</p>
<p class="tx" id="ji_1793" lang="en-US">Mark Zuckerberg saw from the very beginning that key to Facebook’s success would be its ability to be a vehicle, or in fact even a manufacturer, of a “social web,” in which people would engage in a range of social activities. To accomplish this, he prioritized the growth of the platform above all else.</p>
<p class="tx" id="ji_1794" lang="en-US">But monetizing this information was always going to be a challenge, even with Google’s successful business model out there to be emulated. Facebook’s first few forays into data collection as a means of improving its ability to target ads were failures. In 2007 the company introduced a program called Beacon as a way of scooping up information about Facebook users’ purchases on other sites and then sharing it with their friends on their newsfeed. The initiative was immediately seen to be a colossal violation of user privacy and was discontinued. The company needed to forge an approach that combined a massive amount of data collection for digital advertisements and at least some amount of user control.</p>
<p class="tx" id="ji_1795" lang="en-US">The person who made this a reality was Sheryl Sandberg, who had been in charge of Google’s AdWords and had been instrumental in the transformation of that company into a targeted-ad machine. In 2008 she was hired at Facebook as chief operating officer. Sandberg understood how to make this combination work and also the potential that Facebook had in this space: the company could create new demand for products, and thus for advertising, by leveraging its knowledge about users’ social circles <a id="page-369"></a>and preferences. Already in November 2008, Sandberg summarized this combination as foundational to the company’s growth, stating that “what we believe we’ve done is we’ve taken the power of real trust, real user privacy controls, and made it possible for people to be their authentic selves online.” If people were their authentic selves, then they would reveal more about themselves, and there would be more information to be used for generating ad revenue.</p>
<p class="tx" id="ji_1796" lang="en-US">The first important innovation in this effort was the “Like” button, which not only revealed much more about user preferences but also would act as an emotional cue to encourage greater engagement. Several other architectural changes—for example, concerning how the newsfeed works and how users can give feedback—were also introduced. Most importantly, AI algorithms started organizing each user’s newsfeed to attract and retain their attention and, of course, place ads in the most profitable manner.</p>
<p class="tx" id="ji_1797" lang="en-US">Facebook also began offering new tools to advertisers, again based on basic AI technologies. These included the ability to build custom audiences so that ads could be sent to users with certain specific demographics, and capabilities to form look-alike audiences, which Facebook itself describes as “a way your ads can reach new people who are likely to be interested in your business because they share similar characteristics to your existing customers.”</p>
<p class="tx" id="ji_1798" lang="en-US">Social media’s great advantage over search engines when it came to ads was intense engagement. People sometimes pay attention to ads when they search for a product or shop using an engine such as Google, but this is a short engagement, and the amount that the company can make by selling ads is correspondingly limited. If people were to spend more time watching what pops up on their screens, that would mean greater ad revenues. Working to increase Likes for posts from friends and acquaintances proved a great way of boosting such engagement.</p>
<p class="tx" id="ji_1799" lang="en-US">From its early days, Facebook played with people’s psychology for achieving these objectives, and in fact engaged in systematic testing and experiments with its users to determine which types <a id="page-370"></a>of posts and which ways of presenting them would generate more emotion and reaction.</p>
<p class="tx" id="ji_1800" lang="en-US">Social relations, especially within groups, are always fraught with feelings of disapproval, rejection, and envy. There is now abundant evidence that Facebook triggers not just outrage at political content but also strong negative emotions in other social contexts. It then exploits all these emotions to encourage people to spend more time on the platform. Sensational content makes people spend more time on the platform, as does anxiety. Several social psychology studies show that social media use is intertwined with feelings of envy and inadequacy, and often leads to concerns about self-esteem.</p>
<p class="tx" id="ji_1801" lang="en-US">The expansion of Facebook across US college campuses, for example, had a powerful negative impact on mental health, often leading to feelings of depression. Students whose campus gained access to the platform also started reporting significantly worse academic performance, indicating that the effects are not confined to emotions but affect off-line behavior as well. Facebook powerfully monetizes these feelings because both anxieties and efforts to gain greater approval increase the time that people spend on the platform.</p>
<p class="tx" id="ji_1802" lang="en-US">An ambitious research project is revealing about this issue. Researchers incentivized some people on Facebook to (temporarily) give up using the platform and then compared their time use and emotional states to members of a control group who were given no such inducement and continued to use Facebook intensively. Those who were encouraged to stop using Facebook spent more time doing other social activities and were significantly happier. But, reflecting the social pressure that they might have felt from peers and from the platform trying to reengage them, when the study was over, they went back to Facebook—worse mental state and all.</p>
<p class="tx" id="ji_1803" lang="en-US">In the service of increasing user engagement, many new features and algorithms at Facebook were introduced rapidly and without much study of how they would affect user psychology and misinformation on the platform. The general approach of the company and its engineers toward introducing new features <a id="page-371"></a>aimed at increasing user engagement is summarized by “Fuck it, ship it,” an expression used frequently by its employees.</p>
<p class="tx" id="ji_1804" lang="en-US">But it was not just a case of unintentional damages on the way to achieve greater engagement. Facebook leadership was intent on maximizing user engagement and did not want other considerations to stand in the way. Sandberg repeatedly insisted that there should be more ads on Instagram, which had been acquired by Facebook in 2012 with promises that the app would remain independent from Facebook and make its own business decisions, including about the design of the app and about advertisements.</p>
<p class="tx" id="ji_1805" lang="en-US">When Facebook had decided to change its algorithm so that it would not promote misleading stories and untrustworthy websites after the 2020 US presidential election, the results were striking. Hateful content and misinformation stopped going viral. But a short while later, the changes were reversed, and the platform was back to business as usual, largely because when the company tested the effect of the change on engagement, it found that when people were getting less enraged and triggered, they were spending less time there.</p>
<p class="tx" id="ji_1806" lang="en-US">Throughout, Zuckerberg and Sandberg, later joined by Clegg, defended these decisions on the basis that the platform should not limit anyone’s free speech. In response, the British comedian Sacha Baron Cohen summarized what many thought was the problem: “This is about giving people, including some of the most reprehensible people on earth, the biggest platform in history to reach a third of the planet.”</p>
<p class="h1" id="ji_1807" lang="en-US">The Antidemocratic Turn</p>
<p class="cotx" id="ji_1808" lang="en-US">We cannot understand the political mess that social media has created without recognizing the profit motive based on targeted ads, which makes these companies prioritize maximizing user engagement and sometimes rage. Targeted advertisements, in turn, would not have been possible without the collection and processing of massive amounts of data.</p>
<p class="tx" id="ji_1809" lang="en-US">The profit motive is not the only factor that has pushed the tech industry in this antidemocratic direction. These companies’ <a id="page-372"></a>founding vision, which we dubbed the AI illusion, has played an equally important role.</p>
<p class="tx" id="ji_1810" lang="en-US">As we discussed in <a href="009_Chapter_002.xhtml">Chapter 1</a>, democracy, above all else, is about a multitude of voices, critically including those of ordinary people, being heard and becoming significant in public-policy directions. The notion of the “public sphere,” proposed by the German philosopher Jürgen Habermas, captures some of the essential features of healthy democratic discourse. Habermas argued that the public sphere, defined as forums where individuals form new associations and discuss social issues and policy, is pivotal for democratic politics. Using British coffeehouses or French salons of the nineteenth century as the model, Habermas suggested that the critical ingredient of the public sphere is the ability that it offers to people to freely participate in debates on issues of general interest without a strict hierarchy based on preexisting status. In this way, the public sphere generates both a forum for diverse opinions to be heard and a springboard for these opinions to influence policy. It can be particularly effective when it allows people to interact with others on a range of cross-cutting issues.</p>
<p class="tx" id="ji_1811" lang="en-US">Early on, there was even a hope that online communications could generate a new public sphere, one where people from even more diverse backgrounds than in local politics could freely interact and exchange opinions.</p>
<p class="tx" id="ji_1812" lang="en-US">Unfortunately, online democracy is not in line with the business models of leading tech companies and the AI illusion. In fact, it is diametrically opposed to a technocratic approach, which maintains that many important decisions are too complex for regular people. The vibe in the corridors of most tech companies is that men (and sometimes, but not that often, women) of genius are at work, striving for the common good. It is only natural that they should be the ones making the important decisions. When approached this way, the political discourse of the masses becomes something to be manipulated and harvested, not something to be encouraged and protected.</p>
<p class="tx" id="ji_1813" lang="en-US">The AI illusion thus favors an antidemocratic impulse, even as many of its executives view themselves to be on the center-left <a id="page-373"></a>and supporters of democratic institutions and even the Democratic Party. Their support is often rooted in cultural issues and conveniently bypasses the vital building block of democracy: people’s active participation in politics. Such participation is especially discouraged when it comes to AI because most entrepreneurs and venture capitalists believe that people do not understand the technology and unnecessarily worry about its intrusive effects. As one venture capitalist put it, “Most of the fears of artificial intelligence are overblown if not altogether unfounded.” The solution is to ignore these concerns, forge ahead, and integrate AI into every aspect of our lives because “perhaps it’s only when a technology is fully integrated into daily life, and recedes into the background of our imagination, that people stop fearing it.” This was essentially the same approach advocated by Mark Zuckerberg when he told<span class="ital" lang=""> Time</span> magazine, “Whenever any technology or innovation comes along and it changes the nature of something, there are always people who lament the change and wish to go back to the previous time. But, I mean, I think that it’s so clearly positive for people in terms of their ability to stay connected to folks.”</p>
<p class="tx" id="ji_1814" lang="en-US">Another aspect of the AI illusion, the elevation of disruption as a virtue encapsulated by “move fast and break things,” has accelerated this antidemocratic turn. <span class="ital" lang="">Disruption</span> came to mean any negative effects on others, including workers, civil society organizations, traditional media, or even democracy. It was all fair game, in fact encouraged, so long as it was a consequence of exciting new technologies and consistent with bigger market share and moneymaking.</p>
<p class="tx" id="ji_1815" lang="en-US">A reflection of this antidemocratic impulse can be seen in Facebook’s own research on how users respond to negative and positive emotions from friends in their newsfeed. In 2014 the company undertook a massive internal study, manipulating the newsfeed of nearly seven hundred thousand users by reducing their exposure to either positive or negative expressions for a week. Unsurprisingly, greater exposure to negative emotions and lower exposure to positive emotions impacted users, with lasting adverse effects.</p>
<p class="tx" id="ji_1816" lang="en-US"><a id="page-374"></a>The company did not ask for any permission for this massive study from the users or even attempt to adhere to commonly accepted standards in scientific research, where informed consent from subjects is necessary. After some of the results of the study were published by Facebook researchers and others in the<span class="ital" lang=""> Proceedings of the National Academy of Sciences</span>, the editor in chief published an Editorial Expression of Concern because the study was done without informed consent and did not meet accepted standards of academic research. Google followed the same playbook in its efforts to expand the amount of information that it collected with Google Books and Google Maps. The company ignored privacy concerns and acted first, without permission or consultation, hoping that things would get sorted out or, at the very least, its fait accompli would be accepted. That worked out, at least for Google.</p>
<p class="tx" id="ji_1817" lang="en-US">Facebook and Google are not exceptional in the industry. It is now routine for tech companies to collect vast amounts of data without any consent from the people whose information or photos are being harnessed. In the area of image recognition, for example, many AI algorithms are trained and sometimes take part in competitions on the ImageNet data set, initiated by the computer scientist and later chief scientist of Google Cloud, Fei-Fei Li. The data set, which contains more than 15 million images sorted into more than 22,000 categories, was built by collecting private photos uploaded to various applications on the internet, with no permission from the people who took or appear in these pictures. This was generally viewed as acceptable in the tech industry. In Li’s assessment, “In the age of the Internet, we are suddenly faced with an explosion in terms of imagery data.”</p>
<p class="tx" id="ji_1818" lang="en-US">According to reporting in the <span class="ital" lang="">New York Times</span>, Clearview has systematically collected facial images without consent, aiming to build predictive tools that identify illegal immigrants and people likely to commit crimes. Such strategies are justified by arguing that large-scale data collection is necessary for technological advancement. As an investor in a facial-recognition start-up summed up, the defense for massive data collection is that “laws <a id="page-375"></a>have to determine what’s legal, but you can’t ban technology. Sure, that might lead to a dystopian future or something, but you can’t ban it.”</p>
<p class="tx" id="ji_1819" lang="en-US">The truth is more nuanced. Imposing massive surveillance and data collection is not the only path of technological advance, and limiting it does not mean banning technology. What we are experiencing instead is an antidemocratic trajectory charted by the profit motive and the AI illusion, which involves authoritarian governments and tech companies foisting their vision on everybody else.</p>
<p class="h1" id="ji_1820" lang="en-US">Radio Days</p>
<p class="cotx" id="ji_1821" lang="en-US">Perhaps all of these issues are not specific to digital technologies and AI. Every great new communication technology contains the potential for abuse.</p>
<p class="tx" id="ji_1822" lang="en-US">Consider another one of the transformative communication technologies of the twentieth century: radio. Radio is also a general-purpose technology and, in its way, was as revolutionary as social media, allowing for the first time in history different forms of entertainment, mass broadcasting of information, and, of course, propaganda. The technology was developed soon after the German physicist Heinrich Hertz proved the existence of radio waves in 1886, and the first radio transmitters were built by the Italian physicist Guglielmo Marconi a decade later. By the early 1900s, there were radio broadcasts, and during the 1920s commercial radio became widespread in many Western nations. Propaganda and misinformation started almost immediately. President Franklin D. Roosevelt understood the importance of the technology and made his fireside chats on live radio a key part of the efforts to explain his New Deal policies to the American public.</p>
<p class="tx" id="ji_1823" lang="en-US">An early supporter of FDR came to be identified with radio propaganda in the United States: Father Charles Coughlin, a Roman Catholic priest with great oratory skills. By the mid-1930s, however, Father Coughlin had turned against New Deal policies and founded the National Union for Social Justice. His <a id="page-376"></a>radio speeches, initially broadcast on the CBS network, focused as much on anti-Semitic propaganda as on his policy ideas. Father Coughlin was soon supporting Benito Mussolini and Adolf Hitler on the airways.</p>
<p class="tx" id="ji_1824" lang="en-US">Coughlin’s blend of anti-FDR, fascist, and anti-Semitic broadcasts had major effects on US politics in the 1930s. Recent research used differences across US counties in the strength of radio signals, determined by geographic and topological obstacles to transmission, to investigate this issue. It finds that Father Coughlin’s radio propaganda reduced support for New Deal policies and depressed FDR’s vote in the 1936 presidential election by several percentage points (even if it could not prevent his landslide victory). It was not just presidential votes that Coughlin influenced. Counties receiving his broadcasts uninterrupted were more likely to open a local branch of the pro-Nazi German-American Bund and to lend less support to America’s World War II effort. Several decades later, they still exhibited more anti-Jewish feelings.</p>
<p class="tx" id="ji_1825" lang="en-US">What Father Coughlin effectively exploited in the United States was perfected in Germany at the same time. The Nazis, once in power, heavily relied on radio propaganda. Hitler’s propaganda minister, Joseph Goebbels, became an expert at using the airways to whip up support for Nazi policies and hatred against Jewish people and “Bolsheviks.” Goebbels himself said that “our way of taking power and using it would have been inconceivable without the radio and the airplane.”</p>
<p class="tx" id="ji_1826" lang="en-US">Nazis were indeed quite effective at manipulating sentiments with radio broadcasts. Exploring once again variations in the strength of radio signals across different parts of Germany, as well as changes in radio broadcast content over time, a team of researchers found powerful effects from Nazi propaganda. These radio broadcasts increased anti-Semitic activities and denunciations of Jews to authorities.</p>
<p class="tx" id="ji_1827" lang="en-US">Radio propaganda by extremists was ultimately brought under control in the United States and Germany, and how this was done is revealing about the differences between social media and radio. <a id="page-377"></a>It also suggests some lessons for how new communication technologies can be best used.</p>
<p class="tx" id="ji_1828" lang="en-US">The problem in the 1930s was that Father Coughlin had a national platform to reach millions with inflammatory rhetoric. The problem today is that misinformation is propagated by Facebook and other social media sites’ algorithms to reach potentially billions of people.</p>
<p class="tx" id="ji_1829" lang="en-US">Coughlin’s pernicious effects were neutralized when FDR’s administration decided that the First Amendment protected free speech but not the right to broadcast. It argued that radio spectrum was a publicly owned commons that must be regulated. With new regulations requiring broadcasting permits, Father Coughlin’s programs were forced off the air. Coughlin continued to write and soon started broadcasting again, though with more limited access and only through individual stations. His antiwar, pro-German propaganda was further curtailed after the outbreak of World War II.</p>
<p class="tx" id="ji_1830" lang="en-US">Today, there is plenty of misinformation and hate speech on AM talk shows, but they do not have the reach that Father Coughlin’s national broadcasts achieved or the kind of platform that Facebook’s algorithms provide for online misinformation.</p>
<p class="tx" id="ji_1831" lang="en-US">The postwar German reaction to radio propaganda was even more comprehensive. The German Constitution bans speech classified as <span class="ital" lang="">Volksverhetzung</span>, meaning “incitement to hatred,” as well as incitement to violence or acts denying the dignity of certain segments of the population. Under this law, denying the Holocaust and spreading incendiary anti-Jewish propaganda are outlawed.</p>
<p class="h1" id="ji_1832" lang="en-US">Digital Choices</p>
<p class="cotx" id="ji_1833" lang="en-US">AI technologies did not have to focus on automating work and monitoring employees in workplaces. Nor did they have to be developed to empower government censorship. There is also nothing inherently antidemocratic in digital technologies, and social media certainly does not have to focus on maximizing outrage, <a id="page-378"></a>extremism, and indignation. It was a matter of choice—choice by tech companies, AI researchers, and governments—that got us into our current predicament.</p>
<p class="tx" id="ji_1834" lang="en-US">As we mentioned earlier in this chapter, YouTube and Reddit were initially as much afflicted by far-right extremism, misinformation, and hate speech as Facebook was. But over the last five years, these two platforms took some steps to lessen the problem.</p>
<p class="tx" id="ji_1835" lang="en-US">As public pressure mounted on YouTube and its parent company, Google, after insider accounts such as Caleb Cain’s and exposés in the <span class="ital" lang="">New York Times</span> and the<span class="ital" lang=""> New Yorker</span> came out, the platform started modifying its algorithms to reduce the spread of the most malicious content. Google now claims that it promotes videos from “authoritative sources,” which are less likely to be used for radicalization or contain misinformation. It also states that these algorithmic adjustments have reduced the viewing of “borderline content” by 70 percent (“borderline” here refers to the fact that the company maintains that hate speech is already vetted out).</p>
<p class="tx" id="ji_1836" lang="en-US">The story of Reddit is similar. Home to some of the worst extremist and incendiary material, initially defended by one of its founders, Steve Huffman, as being completely consistent with the “open and honest discussion” philosophy of the site, the platform has since responded to public pressure and tightened its moderation standards. After the 2017 white supremacist Unite the Right rally in Charlottesville, Virginia, fueled and organized on the platform, turned violent and killed a counterprotester and injured dozens of others, Reddit’s founders and the platform had an about-face. The platform started removing scores of sub-Reddits advocating hate speech, racist language, and blatant misinformation. In 2019 it removed The_Donald.</p>
<p class="tx" id="ji_1837" lang="en-US">Improvements resulting from self-regulation by the platforms should not be exaggerated. There is still plenty of misinformation and manipulation, often aided by algorithms on YouTube, and plenty of hateful content on Reddit. Neither platform has changed its business model, and for the most part both platforms continue to rely on maximizing engagement and targeted-ad <a id="page-379"></a>revenues. Platforms that have different business models, such as Uber and Airbnb, have been much more proactive in banning hate speech from their websites.</p>
<p class="tx" id="ji_1838" lang="en-US">But the best demonstration of the viability of alternative models comes from Wikipedia. The platform is one of the most visited services on the web, having received more than 5.5 billion unique annual visitors over the last few years. Wikipedia does not try to monopolize user attention because it does not finance itself by advertisements.</p>
<p class="tx" id="ji_1839" lang="en-US">This has allowed the platform to develop a very different approach to misinformation. Entries in this online encyclopedia are written by anonymous volunteers, and any volunteer editor can start a new entry or modify an existing one. The platform has several layers of administrators, promoted from frequent users with good track records. Among the volunteer contributors, there are experienced editors with additional privileges and responsibilities, such as maintenance or dispute resolution. At a higher level, “stewards” have greater authority to deal with disagreements. According to the platform itself, stewards are “tasked with technical implementation of community consensus, dealing with emergencies, intervening against cross-wiki vandalism.” Above stewards is the “Arbitration Committee,” consisting of “volunteer editors who act in concert or in subgroups imposing binding solutions on conduct disputes the community has been unable to resolve.” “Administrators” have the ability to protect and delete pages, and block editing in case of disputed content or past occurrence of vandalism or misinformation. Administrators themselves are overseen and promoted by “bureaucrats.”</p>
<p class="tx" id="ji_1840" lang="en-US">This administrative structure is instrumental in the site’s ability to prevent the propagation of misinformation and the type of polarization that has been all too common on other sites. Wikipedia’s experience suggests that the wisdom of the crowd, so dearly admired by early techno-optimists of social media, can work, but only when underpinned and monitored by the right organizational structure and when appropriate choices are made on the use and direction of technology.</p>
<p class="tx" id="ji_1841" lang="en-US"><a id="page-380"></a>Alternatives to the targeted-ad business model are not confined to nonprofits such as Wikipedia. Netflix, based on a subscription model, also collects information about users and invests heavily in AI in order to make individual-specific recommendations. But there is little misinformation or political outrage on the platform, for its goal is to improve user experience to encourage subscriptions, not to ensure maximal engagement.</p>
<p class="tx" id="ji_1842" lang="en-US">Social media platforms can work with and make money from a subscription model as well. Such a model will not remedy all of the problems of social media. People can create their own echo chambers in a subscription-based platform, and new ways of monetizing misinformation and insecurities may arise. Nevertheless, alternative business models can move away from seeking intense user engagement, which has proved to be conducive to the worst type of social interaction, damaging both to mental health and democratic discourse.</p>
<p class="tx" id="ji_1843" lang="en-US">A “social web” can have myriad positive effects if its pernicious impact on misinformation, polarization, and mental health can be contained. Recent research tracks the start of Facebook’s service in new languages and shows that small businesses in the affected countries get access to information from foreign markets and expand their sales as a result. There is no reason to believe that a company could not make money on the basis of these types of services rather than on its ability to manipulate users. Social media and digital tools can also provide greater protection to individuals against surveillance and can even play a pro-democratic role, as we will discuss in <a href="019_Chapter_012.xhtml">Chapter 11</a>. Pushing buttons for emotional reactions and targeting ads to users when they are thus triggered were never the only options for social media.</p>
<p class="h1" id="ji_1844" lang="en-US">Democracy Undermined When We Most Need It</p>
<p class="cotx" id="ji_1845" lang="en-US">The tragedy is that AI is further undermining democracy when we need it most. Unless the direction of digital technologies is altered fundamentally, they will continue to fuel inequality and marginalize large segments of the labor force, both in the West <a id="page-381"></a>and increasingly around the world. AI technologies are also being used to more intensively monitor workers and, through this channel, create even more downward pressure on wages.</p>
<p class="tx" id="ji_1846" lang="en-US">You can pin your hopes on the productivity bandwagon if you like. But there is no indication that shared productivity gains will be forthcoming soon. As we have seen, managers and entrepreneurs often have a bias to use new technologies to automate work and disempower people, unless reined in by countervailing powers. Massive data collection has exacerbated this bias.</p>
<p class="tx" id="ji_1847" lang="en-US">Countervailing powers are hard to come by without democracy, however. When an elite completely controls politics and can use tools of repression and propaganda effectively, it is hard to build any meaningful, well-organized opposition. So robust dissent will not rise in China anytime soon, especially under the increasingly effective system of censorship and AI-based surveillance that the Communist Party has established. But it is also becoming increasingly difficult to hope for the resurgence of countervailing powers in the United States and much of the rest of the Western world. AI is choking democracy while also providing the tools for repression and manipulation to both authoritarian and democratically elected governments.</p>
<p class="tx" id="ji_1848" lang="en-US">As George Orwell asked in <span class="ital" lang="">1984</span>, “For, after all, how do we know that two and two make four? Or that the force of gravity works? Or that the past is unchangeable? If both the past and the external world exist only in the mind, and if the mind itself is controllable, what then?” This question is even more relevant today because, as philosopher Hannah Arendt anticipated, when bombarded with falsehoods and propaganda, people both in democratic and nondemocratic countries stop believing any news. It may be even worse than that. Glued to their social media and frequently outraged and very often absorbed by strong emotions, people may become divorced from their community and democratic discourse because an alternative, segregated reality has been created online, where extremist voices are loudest, artificial echo chambers abound, all information is suspect or partisan, and compromise has been forgotten or even condemned.</p>
<p class="tx" id="ji_1849" lang="en-US"><a id="page-382"></a>Some are optimistic that new technologies, such as Web 3.0 or the metaverse, can provide different dynamics. But as long as the current business model of tech companies and the surveillance obsession of governments prevail, they are more likely to further exacerbate these trends, creating even more powerful filter bubbles and a wider wedge with reality.</p>
<p class="tx" id="ji_1850" lang="en-US">It is late, but perhaps not too late. <a href="019_Chapter_012.xhtml">Chapter 11</a> outlines how the tide can be turned and which specific policy proposals hold promise for such a transformation.</p>
</section>
</div>



  </div>

  
  <div class="calibreToc">
    <h2><a href="../../../Power and Progress Our Thousand-Year Struggle Over Technology and Prosperity (Daron Acemoglu Simon Johnson) (Z-Library).html">Table of contents
</a></h2>
    <div>
  <ul>
    <li>
      <a href="003_Title.xhtml">Title Page</a>
    </li>
    <li>
      <a href="004_Copyright.xhtml">Copyright</a>
    </li>
    <li>
      <a href="005_Dedication.xhtml">Dedication</a>
    </li>
    <li>
      <a href="008_Chapter_001.xhtml">Prologue: What Is Progress?</a>
    </li>
    <li>
      <a href="009_Chapter_002.xhtml">1 Control over Technology</a>
    </li>
    <li>
      <a href="010_Chapter_003.xhtml">2 Canal Vision</a>
    </li>
    <li>
      <a href="011_Chapter_004.xhtml">3 Power to Persuade</a>
    </li>
    <li>
      <a href="012_Chapter_005.xhtml">4 Cultivating Misery</a>
    </li>
    <li>
      <a href="013_Chapter_006.xhtml">5 A Middling Sort of Revolution</a>
    </li>
    <li>
      <a href="014_Chapter_007.xhtml">6 Casualties of Progress</a>
    </li>
    <li>
      <a href="015_Chapter_008.xhtml">7 The Contested Path</a>
    </li>
    <li>
      <a href="016_Chapter_009.xhtml">8 Digital Damage</a>
    </li>
    <li>
      <a href="017_Chapter_010.xhtml">9 Artificial Struggle</a>
    </li>
    <li>
      <a href="018_Chapter_011.xhtml">10 Democracy Breaks</a>
    </li>
    <li>
      <a href="019_Chapter_012.xhtml">11 Redirecting Technology</a>
    </li>
    <li>
      <a href="020_Bm.xhtml">Photos</a>
    </li>
    <li>
      <a href="054_Bm.xhtml">Bibliographic Essay</a>
    </li>
    <li>
      <a href="055_Bm.xhtml">References</a>
    </li>
    <li>
      <a href="056_Bm.xhtml">Acknowledgments</a>
    </li>
    <li>
      <a href="discover-page.xhtml">Discover More</a>
    </li>
    <li>
      <a href="057_Bm.xhtml">Image Credits</a>
    </li>
    <li>
      <a href="058_Bm.xhtml">About the Authors</a>
    </li>
    <li>
      <a href="002_ad-card.xhtml">Also by Daron Acemoglu</a>
    </li>
    <li>
      <a href="002_ad-card.xhtml#toc_2b">Also by Simon Johnson</a>
    </li>
    <li>
      <a href="001_Fm.xhtml">Praise for "Power and Progress"</a>
    </li>
  </ul>
</div>


  </div>
  

  <div class="calibreEbNav">
    
      <a href="017_Chapter_010.xhtml" class="calibreAPrev">previous page
</a>
    

    <a href="../../../Power and Progress Our Thousand-Year Struggle Over Technology and Prosperity (Daron Acemoglu Simon Johnson) (Z-Library).html" class="calibreAHome">start
</a>

    
      <a href="019_Chapter_012.xhtml" class="calibreANext">next page
</a>
    
  </div>

</div>

</body>
</html>
